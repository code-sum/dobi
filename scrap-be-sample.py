import requests
from bs4 import BeautifulSoup
import time
import re

class TextCrawler:
    def __init__(self, search_terms=['apple', 'banana'], delay=1):
        self.search_terms = search_terms
        self.delay = delay  # ìš”ì²­ ê°„ ì§€ì—° ì‹œê°„ (ì´ˆ)
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        self.results = []
    
    def fetch_page(self, url):
        """ì›¹ í˜ì´ì§€ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜"""
        try:
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            return response.text
        except requests.RequestException as e:
            print(f"âŒ {url} í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def search_text_in_content(self, content, url):
        """HTML ì½˜í…ì¸ ì—ì„œ ê²€ìƒ‰ì–´ë¥¼ ì°¾ëŠ” í•¨ìˆ˜"""
        soup = BeautifulSoup(content, 'html.parser')
        
        # ìŠ¤í¬ë¦½íŠ¸ì™€ ìŠ¤íƒ€ì¼ íƒœê·¸ ì œê±°
        for script in soup(["script", "style"]):
            script.decompose()
        
        # í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
        text = soup.get_text()
        text = ' '.join(text.split())  # ê³µë°± ì •ë¦¬
        
        found_terms = []
        for term in self.search_terms:
            # ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´ ê²€ìƒ‰
            if re.search(rf'\b{re.escape(term)}\b', text, re.IGNORECASE):
                found_terms.append(term)
        
        return found_terms, text
    
    def crawl_sites(self, urls):
        """ì—¬ëŸ¬ ì‚¬ì´íŠ¸ë¥¼ í¬ë¡¤ë§í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜"""
        print(f"ğŸ” ê²€ìƒ‰ì–´: {', '.join(self.search_terms)}")
        print(f"ğŸ“ ì´ {len(urls)}ê°œ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì‹œì‘...\n")
        
        for i, url in enumerate(urls, 1):
            print(f"[{i}/{len(urls)}] í¬ë¡¤ë§ ì¤‘: {url}")
            
            # í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
            content = self.fetch_page(url)
            if content is None:
                continue
            
            # í…ìŠ¤íŠ¸ ê²€ìƒ‰
            found_terms, page_text = self.search_text_in_content(content, url)
            
            # ê²°ê³¼ ì €ì¥
            result = {
                'url': url,
                'found_terms': found_terms,
                'found_count': len(found_terms),
                'page_title': self.extract_title(content),
                'preview_text': page_text[:200] + '...' if len(page_text) > 200 else page_text
            }
            
            self.results.append(result)
            
            # ê²°ê³¼ ì¶œë ¥
            if found_terms:
                print(f"âœ… ë°œê²¬: {', '.join(found_terms)}")
            else:
                print("âŒ ê²€ìƒ‰ì–´ ì—†ìŒ")
            
            print("-" * 50)
            
            # ìš”ì²­ ê°„ ì§€ì—°
            if i < len(urls):
                time.sleep(self.delay)
    
    def extract_title(self, content):
        """í˜ì´ì§€ ì œëª© ì¶”ì¶œ"""
        soup = BeautifulSoup(content, 'html.parser')
        title_tag = soup.find('title')
        return title_tag.text.strip() if title_tag else "ì œëª© ì—†ìŒ"
    
    def print_summary(self):
        """í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ“Š í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½")
        print("="*60)
        
        total_sites = len(self.results)
        sites_with_terms = len([r for r in self.results if r['found_count'] > 0])
        
        print(f"ì´ í¬ë¡¤ë§ëœ ì‚¬ì´íŠ¸: {total_sites}ê°œ")
        print(f"ê²€ìƒ‰ì–´ê°€ ë°œê²¬ëœ ì‚¬ì´íŠ¸: {sites_with_terms}ê°œ")
        print(f"ì„±ê³µë¥ : {sites_with_terms/total_sites*100:.1f}%\n" if total_sites > 0 else "")
        
        # ê²€ìƒ‰ì–´ë³„ í†µê³„
        term_stats = {term: 0 for term in self.search_terms}
        for result in self.results:
            for term in result['found_terms']:
                term_stats[term] += 1
        
        print("ê²€ìƒ‰ì–´ë³„ ë°œê²¬ íšŸìˆ˜:")
        for term, count in term_stats.items():
            print(f"  â€¢ {term}: {count}ê°œ ì‚¬ì´íŠ¸")
        
        # ìƒì„¸ ê²°ê³¼
        print(f"\n{'='*60}")
        print("ğŸ“‹ ìƒì„¸ ê²°ê³¼")
        print("="*60)
        
        for i, result in enumerate(self.results, 1):
            print(f"\n[{i}] {result['url']}")
            print(f"ì œëª©: {result['page_title']}")
            if result['found_terms']:
                print(f"ë°œê²¬ëœ ê²€ìƒ‰ì–´: {', '.join(result['found_terms'])}")
            else:
                print("ë°œê²¬ëœ ê²€ìƒ‰ì–´: ì—†ìŒ")
    
    def save_results_to_file(self, filename="crawling_results.txt"):
        """ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        with open(filename, 'w', encoding='utf-8') as f:
            f.write("ì›¹ í¬ë¡¤ë§ ê²°ê³¼\n")
            f.write("="*50 + "\n\n")
            f.write(f"ê²€ìƒ‰ì–´: {', '.join(self.search_terms)}\n")
            f.write(f"í¬ë¡¤ë§ ì¼ì‹œ: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            for i, result in enumerate(self.results, 1):
                f.write(f"[{i}] {result['url']}\n")
                f.write(f"ì œëª©: {result['page_title']}\n")
                f.write(f"ë°œê²¬ëœ ê²€ìƒ‰ì–´: {', '.join(result['found_terms']) if result['found_terms'] else 'ì—†ìŒ'}\n")
                f.write(f"ë¯¸ë¦¬ë³´ê¸°: {result['preview_text']}\n")
                f.write("-" * 50 + "\n")
        
        print(f"ğŸ’¾ ê²°ê³¼ê°€ '{filename}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


def main():
    # í¬ë¡¤ë§í•  ì‚¬ì´íŠ¸ ëª©ë¡
    urls_to_crawl = [
        'https://www.wikipedia.org/wiki/Apple',
        'https://www.wikipedia.org/wiki/Banana',
    ]
    
    # í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    crawler = TextCrawler(
        search_terms=['apple', 'banana'],  # ê²€ìƒ‰í•  ë‹¨ì–´ë“¤
        delay=1  # ìš”ì²­ ê°„ ì§€ì—°ì‹œê°„ (ì´ˆ)
    )
    
    # í¬ë¡¤ë§ ì‹¤í–‰
    crawler.crawl_sites(urls_to_crawl)
    
    # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
    crawler.print_summary()

if __name__ == "__main__":
    main()